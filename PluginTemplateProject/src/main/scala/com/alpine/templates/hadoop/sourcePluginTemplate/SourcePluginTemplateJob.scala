package com.alpine.templates.hadoop.sourcePluginTemplate

import com.alpine.plugin.core.{OperatorListener, OperatorParameters}
import com.alpine.plugin.core.io.{HdfsTabularDataset, IONone, TSVAttributes}
import com.alpine.plugin.core.spark.{AlpineSparkEnvironment, SparkIOTypedPluginJob}
import com.alpine.plugin.core.utils.{AddendumWriter, HdfsParameterUtils}
import org.apache.spark.sql.{Row, SQLContext}

/**
 * Source operator template
 * Shows a simple operator that takes an input parameter for [number of rows] and produces an output dataset
 * with that many number of rows.
 * IONone indicates no input dataset required for the operator
 * OperatorListener is used to log messages to status section of the canvas
 * OperatorParameters holds the reference to the widgets on the operator UI
 **/

class SourcePluginTemplateJob extends SparkIOTypedPluginJob[IONone, HdfsTabularDataset] {
  import SourcePluginTemplateUtils._

  // TODO: Add your input parameters and spark code here
  override def onExecution(alpineSparkEnvironment: AlpineSparkEnvironment,
                           input: IONone,
                           params: OperatorParameters,
                           listener: OperatorListener): HdfsTabularDataset = {

    //we create a spark utils object from the spark context generated by the runtime class and
    //provided in this method.
    val sparkUtils = alpineSparkEnvironment.getSparkUtils
    val sc = alpineSparkEnvironment.sparkSession.sparkContext
    val sqlContext = new SQLContext(sc)
    //retrieve the value of the "Number of Things" parameter using the key defined in
    // the DatasetGeneratorUtils class.
    val numberOfRows = params.getIntValue(numberRowsParamKey)
    listener.notifyMessage("number of rows is " + numberOfRows)

    val rowSeq : Seq[Row] = Seq.range(1, numberOfRows+1).map(rowNumber => Row.fromTuple("Thing", rowNumber));

    //create an RDD from the Sequence generated in the previous step
    val rowRDD = sc.parallelize(rowSeq)

    //retrieve the storage parameters using the HdfsParameterUtils class
    val outputPath = HdfsParameterUtils.getOutputPath(params)
    val overwrite = HdfsParameterUtils.getOverwriteParameterValue(params)
    val storageFormat = HdfsParameterUtils.getHdfsStorageFormatType(params)

    //generate the schema for the output with the dataset generator utils class.
    val outputSchema = getOutputSchema(params)

    /*
    Create a Spark DataFrame. We use the rowRDD and a Spark sql schema. The SparkRuntimeUtils
    class provides methods to convert the tabular schema from Alpines format to the SparkSQLSchema.
    By creating one schema and converting it, we insure that the runtime and design time schemas
    will match.
    */
    val outputDF = sqlContext.createDataFrame(
      rowRDD,
      sparkUtils.convertTabularSchemaToSparkSQLSchema(outputSchema));

    /*
     * Use the Spark Utils class to save the DataFrame and create a HdfsTabularDataSet object
     */
    val message = ""
    val addendum =  AddendumWriter.createStandardAddendum(message)

    sparkUtils.saveDataFrame(
      outputPath,
      outputDF,
      storageFormat,
      overwrite,
      addendum,
      TSVAttributes.defaultCSV);

  }
}

